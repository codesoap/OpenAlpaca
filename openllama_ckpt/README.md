Download the checkpoints of OpenLLaMA and place them in this folder.

1. Previewed version of OpenLLaMA trained with 200 billion tokens:
https://huggingface.co/openlm-research/open_llama_7b_preview_200bt/tree/main/open_llama_7b_preview_200bt_transformers_weights/

2. Previewed version of OpenLLaMA trained with 300 billion tokens:
https://huggingface.co/openlm-research/open_llama_7b_preview_300bt/tree/main/open_llama_7b_preview_300bt_transformers_weights/